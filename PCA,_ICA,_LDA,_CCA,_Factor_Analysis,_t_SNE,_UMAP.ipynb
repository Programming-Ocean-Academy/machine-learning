{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Representation and Latent Variable Methods — A Unified Mathematical View\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Principal Component Analysis (PCA)\n",
        "\n",
        "### Mathematical Objective\n",
        "\n",
        "Given a centered data matrix  \n",
        "$$\n",
        "X \\in \\mathbb{R}^{n \\times d},\n",
        "$$\n",
        "\n",
        "PCA solves:\n",
        "\n",
        "$$\n",
        "\\max_{W^\\top W = I} \\ \\mathrm{Var}(XW)\n",
        "$$\n",
        "\n",
        "which is equivalent to:\n",
        "\n",
        "$$\n",
        "\\max \\ \\mathbb{E}\\left[\\|W^\\top x\\|^2\\right].\n",
        "$$\n",
        "\n",
        "### Core Mechanism\n",
        "\n",
        "Covariance matrix:\n",
        "\n",
        "$$\n",
        "\\Sigma = \\mathbb{E}[x x^\\top].\n",
        "$$\n",
        "\n",
        "PCA performs eigen-decomposition of $\\Sigma$ and projects data onto eigenvectors corresponding to the largest eigenvalues.\n",
        "\n",
        "### What PCA Optimizes\n",
        "\n",
        "Uses **second-order statistics only**.\n",
        "\n",
        "Maximizes variance.\n",
        "\n",
        "Minimizes reconstruction error:\n",
        "\n",
        "$$\n",
        "\\min \\ \\|X - X W W^\\top\\|_F^2.\n",
        "$$\n",
        "\n",
        "### Statistical Interpretation\n",
        "\n",
        "Optimal linear estimator under Gaussian assumptions.\n",
        "\n",
        "Equivalent to the Karhunen–Loève Transform.\n",
        "\n",
        "Equivalent to Singular Value Decomposition (SVD).\n",
        "\n",
        "### Strengths\n",
        "\n",
        "Orthogonal, ordered components.\n",
        "\n",
        "Fast, stable, deterministic.\n",
        "\n",
        "Excellent for compression and denoising.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "Blind to non-Gaussian structure.\n",
        "\n",
        "Components are uncorrelated, not independent.\n",
        "\n",
        "No class awareness.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Independent Component Analysis (ICA)\n",
        "\n",
        "### Generative Model\n",
        "\n",
        "$$\n",
        "x = A s\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "s\n",
        "$$\n",
        "are statistically independent latent sources,\n",
        "\n",
        "$$\n",
        "A\n",
        "$$\n",
        "is an unknown mixing matrix.\n",
        "\n",
        "Goal:\n",
        "\n",
        "$$\n",
        "s = W x.\n",
        "$$\n",
        "\n",
        "### Mathematical Objective\n",
        "\n",
        "Maximize non-Gaussianity (since independence implies non-Gaussianity).\n",
        "\n",
        "Common criteria include kurtosis and negentropy.\n",
        "\n",
        "Negentropy formulation:\n",
        "\n",
        "$$\n",
        "J(s) = H(s_{\\text{gauss}}) - H(s).\n",
        "$$\n",
        "\n",
        "### Core Insight\n",
        "\n",
        "Gaussian variables cannot be separated using independence.\n",
        "\n",
        "This explains why PCA cannot solve ICA problems, although ICA commonly uses PCA whitening as a preprocessing step.\n",
        "\n",
        "### What ICA Optimizes\n",
        "\n",
        "Higher-order statistics.\n",
        "\n",
        "Statistical independence (stronger than uncorrelatedness).\n",
        "\n",
        "### Statistical Interpretation\n",
        "\n",
        "Blind source separation.\n",
        "\n",
        "Maximum likelihood estimation under non-Gaussian priors.\n",
        "\n",
        "### Strengths\n",
        "\n",
        "Recovers meaningful physical sources.\n",
        "\n",
        "Works where PCA fails (EEG, audio, finance).\n",
        "\n",
        "### Limitations\n",
        "\n",
        "No intrinsic ordering of components.\n",
        "\n",
        "Sensitive to noise.\n",
        "\n",
        "Requires non-Gaussianity.\n",
        "\n",
        "Identifiable only up to scale and permutation.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Linear Discriminant Analysis (LDA)\n",
        "\n",
        "### Supervised Setting\n",
        "\n",
        "Data with labels:\n",
        "\n",
        "$$\n",
        "y \\in \\{1, \\ldots, C\\}.\n",
        "$$\n",
        "\n",
        "### Objective Function\n",
        "\n",
        "Maximize class separability:\n",
        "\n",
        "$$\n",
        "\\max_W \\ \\frac{W^\\top S_B W}{W^\\top S_W W}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "S_B\n",
        "$$\n",
        "is the between-class scatter matrix,\n",
        "\n",
        "$$\n",
        "S_W\n",
        "$$\n",
        "is the within-class scatter matrix.\n",
        "\n",
        "### Core Mechanism\n",
        "\n",
        "Solve the generalized eigenvalue problem:\n",
        "\n",
        "$$\n",
        "S_B w = \\lambda S_W w.\n",
        "$$\n",
        "\n",
        "### Dimensionality Constraint\n",
        "\n",
        "$$\n",
        "\\text{dim} \\le C - 1.\n",
        "$$\n",
        "\n",
        "### Statistical Interpretation\n",
        "\n",
        "Optimal Bayes classifier when class-conditional distributions are Gaussian with equal covariance.\n",
        "\n",
        "Equivalent to Fisher’s discriminant.\n",
        "\n",
        "### Strengths\n",
        "\n",
        "Explicitly class-aware.\n",
        "\n",
        "Excellent for classification pipelines.\n",
        "\n",
        "Interpretable projection directions.\n",
        "\n",
        "### Limitations\n",
        "\n",
        "Requires labeled data.\n",
        "\n",
        "Assumes homoscedastic Gaussian classes.\n",
        "\n",
        "Poor performance in high-dimensional, low-sample regimes.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Canonical Correlation Analysis (CCA)\n",
        "\n",
        "### Objective\n",
        "\n",
        "Given two views:\n",
        "\n",
        "$$\n",
        "X \\quad \\text{and} \\quad Y,\n",
        "$$\n",
        "\n",
        "CCA solves:\n",
        "\n",
        "$$\n",
        "\\max_{w_x, w_y} \\ \\mathrm{corr}(w_x^\\top X, w_y^\\top Y).\n",
        "$$\n",
        "\n",
        "### What It Optimizes\n",
        "\n",
        "Shared variance across datasets.\n",
        "\n",
        "Does not maximize variance within each dataset independently.\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "Multimodal learning.\n",
        "\n",
        "Audio–video alignment.\n",
        "\n",
        "Text–image correspondence.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Factor Analysis (FA)\n",
        "\n",
        "### Generative Model\n",
        "\n",
        "$$\n",
        "x = \\Lambda z + \\epsilon\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "z \\sim \\mathcal{N}(0, I),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\epsilon \\sim \\mathcal{N}(0, \\Psi),\n",
        "$$\n",
        "\n",
        "and $\\Psi$ is a diagonal noise covariance matrix.\n",
        "\n",
        "### Key Difference from PCA\n",
        "\n",
        "PCA assumes isotropic noise.\n",
        "\n",
        "FA models dimension-specific noise explicitly.\n",
        "\n",
        "### Statistical Interpretation\n",
        "\n",
        "Probabilistic latent variable model.\n",
        "\n",
        "Direct ancestor of Variational Autoencoders.\n",
        "\n",
        "---\n",
        "\n",
        "## Unified Comparison Table\n",
        "\n",
        "| Method | Supervised | Objective | Statistics Used | Orthogonal | Probabilistic | Typical Use |\n",
        "|------|------------|-----------|-----------------|------------|---------------|-------------|\n",
        "| PCA | No | Maximize variance | $$2^\\text{nd}$$ order | Yes | Yes (Gaussian) | Compression, denoising |\n",
        "| ICA | No | Independence | Higher-order | No | Yes | Source separation |\n",
        "| LDA | Yes | Class separation | $$2^\\text{nd}$$ order + labels | No | Yes | Classification |\n",
        "| CCA | No | Cross-correlation | $$2^\\text{nd}$$ order | No | Yes | Multiview learning |\n",
        "| FA | No | Likelihood maximization | $$2^\\text{nd}$$ order + noise | No | Yes | Latent modeling |\n",
        "\n",
        "---\n",
        "\n",
        "## Signal Processing Perspective (Critical)\n",
        "\n",
        "| SSP Concept | PCA | ICA |\n",
        "|------------|-----|-----|\n",
        "| Noise removal | Yes | Limited |\n",
        "| Source separation | No | Yes |\n",
        "| Whitening | Core operation | Preprocessing step |\n",
        "| Identifiability | Yes | Partial |\n",
        "| Physical meaning | Weak | Strong |\n",
        "\n",
        "---\n",
        "\n",
        "## Geometric Interpretation\n",
        "\n",
        "PCA rotates coordinate axes to align with the principal axes of a data ellipsoid.\n",
        "\n",
        "ICA rotates axes to make projections statistically independent.\n",
        "\n",
        "LDA rotates space to maximize separation between class centroids.\n",
        "\n",
        "CCA rotates two spaces to align directions of maximal shared correlation.\n",
        "\n",
        "---\n",
        "\n",
        "## Modern Extensions\n",
        "\n",
        "| Classical Method | Modern Descendant |\n",
        "|-----------------|------------------|\n",
        "| PCA | Kernel PCA, Autoencoders |\n",
        "| ICA | Nonlinear ICA, Contrastive Learning |\n",
        "| LDA | Metric learning, Fisher networks |\n",
        "| FA | Variational Autoencoders, Diffusion latents |\n",
        "\n",
        "---\n",
        "\n",
        "## Final Mental Model\n",
        "\n",
        "PCA sees shape.\n",
        "\n",
        "ICA hears voices.\n",
        "\n",
        "LDA sees labels.\n",
        "\n",
        "CCA sees agreement.\n",
        "\n",
        "FA sees hidden causes.\n"
      ],
      "metadata": {
        "id": "uQxKJbk45Vfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality Reduction and Manifold Visualization — PCA, t-SNE, and UMAP\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Principal Component Analysis (PCA)\n",
        "\n",
        "### Core Nature\n",
        "\n",
        "Linear, global, deterministic.\n",
        "\n",
        "### Mathematical Objective\n",
        "\n",
        "Given centered data matrix $X$:\n",
        "\n",
        "$$\n",
        "\\max_{W^\\top W = I} \\ \\mathrm{Var}(XW)\n",
        "$$\n",
        "\n",
        "Equivalently:\n",
        "\n",
        "$$\n",
        "\\min \\ \\|X - X W W^\\top\\|_F^2.\n",
        "$$\n",
        "\n",
        "### What PCA Preserves\n",
        "\n",
        "Global variance.\n",
        "\n",
        "Euclidean geometry.\n",
        "\n",
        "Large-scale structure.\n",
        "\n",
        "### What PCA Ignores\n",
        "\n",
        "Local neighborhoods.\n",
        "\n",
        "Nonlinear manifolds.\n",
        "\n",
        "Cluster separability.\n",
        "\n",
        "### Statistical Interpretation\n",
        "\n",
        "Optimal linear estimator under Gaussian assumptions.\n",
        "\n",
        "Eigen-decomposition of the covariance matrix.\n",
        "\n",
        "Equivalent to Singular Value Decomposition (SVD).\n",
        "\n",
        "### Strengths\n",
        "\n",
        "Interpretable axes.\n",
        "\n",
        "Stable and fast.\n",
        "\n",
        "Scales well.\n",
        "\n",
        "Reusable embedding (out-of-sample extension is trivial).\n",
        "\n",
        "### Weaknesses\n",
        "\n",
        "Fails on curved manifolds.\n",
        "\n",
        "Poor for visualization of complex clusters.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "### Core Nature\n",
        "\n",
        "Nonlinear, local, probabilistic.\n",
        "\n",
        "### Fundamental Idea\n",
        "\n",
        "Preserve pairwise neighborhood similarity rather than global geometry.\n",
        "\n",
        "High-dimensional similarity:\n",
        "\n",
        "$$\n",
        "p_{ij} \\propto \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma_i^2}\\right).\n",
        "$$\n",
        "\n",
        "Low-dimensional similarity using Student-$t$ distribution:\n",
        "\n",
        "$$\n",
        "q_{ij} \\propto \\frac{1}{1 + \\|y_i - y_j\\|^2}.\n",
        "$$\n",
        "\n",
        "### Objective Function\n",
        "\n",
        "Minimize Kullback–Leibler divergence:\n",
        "\n",
        "$$\n",
        "\\mathrm{KL}(P \\| Q) = \\sum_{i,j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n",
        "$$\n",
        "\n",
        "### What t-SNE Preserves\n",
        "\n",
        "Local neighborhoods.\n",
        "\n",
        "Cluster tightness.\n",
        "\n",
        "### What t-SNE Destroys\n",
        "\n",
        "Global distances.\n",
        "\n",
        "Relative cluster positions.\n",
        "\n",
        "Density information.\n",
        "\n",
        "### Critical Properties\n",
        "\n",
        "Non-parametric.\n",
        "\n",
        "Non-invertible.\n",
        "\n",
        "Stochastic.\n",
        "\n",
        "No meaningful axes.\n",
        "\n",
        "### Common Misinterpretation (Very Important)\n",
        "\n",
        "Distances between clusters in t-SNE visualizations are meaningless.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. UMAP (Uniform Manifold Approximation and Projection)\n",
        "\n",
        "### Core Nature\n",
        "\n",
        "Nonlinear, topological, manifold-based.\n",
        "\n",
        "### Mathematical Foundation\n",
        "\n",
        "UMAP is grounded in:\n",
        "\n",
        "Riemannian geometry.\n",
        "\n",
        "Algebraic topology.\n",
        "\n",
        "Fuzzy simplicial sets.\n",
        "\n",
        "### Key Assumptions\n",
        "\n",
        "Data lies on a low-dimensional manifold.\n",
        "\n",
        "The manifold is locally connected.\n",
        "\n",
        "Local distances are meaningful.\n",
        "\n",
        "### High-Level Objective\n",
        "\n",
        "Construct a fuzzy graph in high-dimensional space.\n",
        "\n",
        "Construct a fuzzy graph in low-dimensional space.\n",
        "\n",
        "Minimize the cross-entropy between them:\n",
        "\n",
        "$$\n",
        "\\min \\ \\mathrm{CE}(\\text{Graph}_{\\text{high}}, \\text{Graph}_{\\text{low}}).\n",
        "$$\n",
        "\n",
        "### What UMAP Preserves\n",
        "\n",
        "Local neighborhoods.\n",
        "\n",
        "Some global structure.\n",
        "\n",
        "Manifold continuity.\n",
        "\n",
        "### Compared to t-SNE\n",
        "\n",
        "More stable.\n",
        "\n",
        "Better global layout.\n",
        "\n",
        "Faster.\n",
        "\n",
        "Supports out-of-sample transform.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Differences (Intuition First)\n",
        "\n",
        "| Aspect | PCA | t-SNE | UMAP |\n",
        "|------|-----|-------|------|\n",
        "| Type | Linear | Nonlinear | Nonlinear |\n",
        "| Geometry | Euclidean | Probabilistic | Topological |\n",
        "| Focus | Global | Local | Local + global |\n",
        "| Axes meaningful | Yes | No | No |\n",
        "| Clusters | Weak | Strong | Strong |\n",
        "| Global distances | Preserved | Destroyed | Partially preserved |\n",
        "| Deterministic | Yes | No | Mostly |\n",
        "| Scales to large data | Yes | Poor | Good |\n",
        "\n",
        "---\n",
        "\n",
        "## Mathematical Comparison (Deeper)\n",
        "\n",
        "| Property | PCA | t-SNE | UMAP |\n",
        "|--------|-----|-------|------|\n",
        "| Objective | Variance maximization | KL divergence | Cross-entropy |\n",
        "| Distance model | Euclidean | Probabilistic | Graph-based |\n",
        "| Linear map | Yes | No | No |\n",
        "| Probabilistic | Gaussian | Yes | Fuzzy |\n",
        "| Manifold-aware | No | Implicit | Explicit |\n",
        "| Out-of-sample | Trivial | No | Yes |\n",
        "\n",
        "---\n",
        "\n",
        "## Geometry Perspective\n",
        "\n",
        "PCA fits a flat plane through the data.\n",
        "\n",
        "t-SNE preserves which points are close to each other.\n",
        "\n",
        "UMAP preserves how local neighborhoods are connected across the manifold.\n",
        "\n",
        "---\n",
        "\n",
        "## When to Use Which (Correctly)\n",
        "\n",
        "### Use PCA When\n",
        "\n",
        "You need compression.\n",
        "\n",
        "You care about variance.\n",
        "\n",
        "You want interpretable dimensions.\n",
        "\n",
        "You plan to feed data into another model.\n",
        "\n",
        "### Use t-SNE When\n",
        "\n",
        "You want exploratory visualization.\n",
        "\n",
        "You care only about local clusters.\n",
        "\n",
        "The dataset is not too large.\n",
        "\n",
        "You accept instability.\n",
        "\n",
        "### Use UMAP When\n",
        "\n",
        "You want visualization with structural fidelity.\n",
        "\n",
        "You want speed and scalability.\n",
        "\n",
        "You want repeatability.\n",
        "\n",
        "You may need to embed new data later.\n",
        "\n",
        "---\n",
        "\n",
        "## A Crucial Best Practice\n",
        "\n",
        "Never run t-SNE or UMAP on raw high-dimensional data.\n",
        "\n",
        "Correct pipeline:\n",
        "\n",
        "$$\n",
        "\\text{Raw} \\ \\rightarrow \\ \\text{PCA (30–100D)} \\ \\rightarrow \\ \\text{t-SNE / UMAP}.\n",
        "$$\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "Noise removal.\n",
        "\n",
        "Better distance estimates.\n",
        "\n",
        "Faster optimization.\n",
        "\n",
        "Improved manifold learning.\n",
        "\n",
        "---\n",
        "\n",
        "## Signal Processing and Statistical Insight\n",
        "\n",
        "| SSP View | PCA | t-SNE | UMAP |\n",
        "|--------|-----|-------|------|\n",
        "| Noise suppression | Strong | Weak | Moderate |\n",
        "| Density modeling | No | Yes | Yes |\n",
        "| Stochastic model | Gaussian | Explicit | Implicit |\n",
        "| Identifiability | High | Low | Medium |\n",
        "\n",
        "---\n",
        "\n",
        "## One-Sentence Mental Models\n",
        "\n",
        "PCA: “Show me the directions of maximum energy.”\n",
        "\n",
        "t-SNE: “Put neighbors together, no matter the cost.”\n",
        "\n",
        "UMAP: “Preserve the manifold’s shape and connectivity.”\n",
        "\n",
        "---\n",
        "\n",
        "## Final Warning (Important)\n",
        "\n",
        "t-SNE and UMAP are visualization tools, not feature extractors.\n",
        "\n",
        "Do not interpret distances, densities, or axes causally.\n"
      ],
      "metadata": {
        "id": "WNW3Ov_g5urO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Comparison Table of Dimensionality Reduction and Representation Methods\n",
        "\n",
        "| Aspect | PCA | ICA | LDA | CCA | Factor Analysis | t-SNE | UMAP |\n",
        "|------|-----|-----|-----|-----|----------------|-------|------|\n",
        "| **Full Name** | Principal Component Analysis | Independent Component Analysis | Linear Discriminant Analysis | Canonical Correlation Analysis | Factor Analysis | t-Distributed Stochastic Neighbor Embedding | Uniform Manifold Approximation and Projection |\n",
        "| **Learning Type** | Unsupervised | Unsupervised | Supervised | Unsupervised (paired data) | Unsupervised | Unsupervised | Unsupervised |\n",
        "| **Primary Goal** | Maximize variance | Maximize independence | Maximize class separability | Maximize cross-correlation | Explain covariance with latent factors | Preserve local neighborhoods | Preserve manifold topology |\n",
        "| **Data Assumption** | Linear structure | Linear mixing of sources | Gaussian classes, equal covariance | Paired views | Latent variables + noise | Manifold, local similarity | Manifold, local connectivity |\n",
        "| **Uses Labels** | No | No | Yes | No | No | No | No |\n",
        "| **Linear / Nonlinear** | Linear | Linear | Linear | Linear | Linear | Nonlinear | Nonlinear |\n",
        "| **Core Mathematics** | Eigen-decomposition / SVD | Higher-order statistics | Generalized eigenproblem | Correlation optimization | Probabilistic latent model | KL divergence minimization | Graph cross-entropy minimization |\n",
        "| **Statistics Used** | Second-order (covariance) | Higher-order | Second-order + labels | Second-order | Second-order + noise model | Probability distributions | Fuzzy topology |\n",
        "| **Objective Function** | Variance / reconstruction error | Non-Gaussianity | Between / within class ratio | Correlation maximization | Likelihood maximization | KL(P‖Q) | Cross-entropy |\n",
        "| **Preserves Global Structure** | Yes | Partially | Yes (class-wise) | Yes | Yes | No | Partially |\n",
        "| **Preserves Local Structure** | Weak | Moderate | Moderate | Weak | Weak | Strong | Strong |\n",
        "| **Orthogonal Components** | Yes | No | No | No | No | No | No |\n",
        "| **Component Ordering** | Yes | No | Yes | Yes | No | No | No |\n",
        "| **Interpretability of Axes** | High | Medium | High | Medium | Medium | None | None |\n",
        "| **Probabilistic Model** | Yes (Gaussian) | Yes | Yes | Yes | Yes | Yes | Implicit |\n",
        "| **Noise Modeling** | Implicit | Weak | Weak | Weak | Explicit | No | No |\n",
        "| **Invertible Mapping** | Yes (linear) | Yes (up to scale/permutation) | Yes | Yes | Yes | No | No |\n",
        "| **Out-of-Sample Extension** | Trivial | Trivial | Trivial | Trivial | Trivial | No | Yes |\n",
        "| **Stability / Determinism** | High | Medium | High | High | High | Low | Medium–High |\n",
        "| **Scalability** | Excellent | Good | Good | Good | Moderate | Poor | Good |\n",
        "| **Main Use Case** | Compression, denoising | Source separation | Classification | Multiview learning | Latent modeling | Visualization | Visualization |\n",
        "| **Common Pitfall** | Misses nonlinear structure | Sensitive to noise | Fails if assumptions break | Requires paired data | Over-assumes Gaussianity | Misinterpreting distances | Misinterpreting global distances |\n",
        "| **SSP Interpretation** | Energy compaction | Blind source separation | Optimal linear classifier | Cross-signal alignment | Latent signal recovery | Neighborhood preservation | Manifold reconstruction |\n",
        "| **Relation to Deep Learning** | Linear autoencoder | Nonlinear ICA | Metric learning | Multimodal models | VAEs | Visualization only | Visualization / embeddings |\n",
        "\n",
        "---\n",
        "\n",
        "## One-Line Conceptual Summary\n",
        "\n",
        "PCA → Global variance  \n",
        "\n",
        "ICA → Independent sources  \n",
        "\n",
        "LDA → Class separation  \n",
        "\n",
        "CCA → Shared structure across views  \n",
        "\n",
        "Factor Analysis → Latent causes + noise  \n",
        "\n",
        "t-SNE → Local neighborhood visualization  \n",
        "\n",
        "UMAP → Manifold topology visualization\n"
      ],
      "metadata": {
        "id": "_9WdbVJb6nRp"
      }
    }
  ]
}